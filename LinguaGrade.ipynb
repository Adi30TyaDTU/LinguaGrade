{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 97919,
          "databundleVersionId": 11694977,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adi30TyaDTU/LinguaGrade/blob/main/LinguaGrade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "aM2-M41XT4fP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "shl_intern_hiring_assessment_path = kagglehub.competition_download('shl-intern-hiring-assessment')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "sKHewvFyT4fQ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "cyMaNHKJT4fQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv')\n",
        "sample_submission = pd.read_csv('/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv')\n",
        "\n",
        "train_audio_path = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train'\n",
        "test_audio_path = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test'\n",
        "\n",
        "# Check data\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:02:44.041396Z",
          "iopub.execute_input": "2025-04-06T15:02:44.041707Z",
          "iopub.status.idle": "2025-04-06T15:03:15.67595Z",
          "shell.execute_reply.started": "2025-04-06T15:02:44.04168Z",
          "shell.execute_reply": "2025-04-06T15:03:15.674997Z"
        },
        "id": "5tOYx76VT4fR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ASR model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForCTC\n",
        "\n",
        "\n",
        "asr_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "asr_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\").to(device)\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    # Load audio file\n",
        "    speech_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Process for ASR\n",
        "    inputs = asr_processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Move to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get logits\n",
        "    with torch.no_grad():\n",
        "        logits = asr_model(**inputs).logits\n",
        "\n",
        "    # Decode\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = asr_processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Example transcription (we'll process all files in batches later)\n",
        "sample_audio = os.path.join(train_audio_path, train_df.iloc[0]['filename'])\n",
        "print(f\"Sample transcription: {transcribe_audio(sample_audio)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:03:47.126745Z",
          "iopub.execute_input": "2025-04-06T15:03:47.127036Z",
          "iopub.status.idle": "2025-04-06T15:03:50.561539Z",
          "shell.execute_reply.started": "2025-04-06T15:03:47.127006Z",
          "shell.execute_reply": "2025-04-06T15:03:50.560314Z"
        },
        "id": "ItOKq9V2T4fR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load language model for embedding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "def get_text_features(text):\n",
        "    # Basic text stats\n",
        "    features = {\n",
        "        'num_words': len(text.split()),\n",
        "        'num_chars': len(text),\n",
        "        'avg_word_length': np.mean([len(word) for word in text.split()]) if len(text.split()) > 0 else 0,\n",
        "        'unique_word_ratio': len(set(text.split())) / len(text.split()) if len(text.split()) > 0 else 0,\n",
        "    }\n",
        "\n",
        "    # Get BERT embeddings\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs)\n",
        "\n",
        "    # Use mean pooling for sentence embedding\n",
        "    mean_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    for i, val in enumerate(mean_embedding):\n",
        "        features[f'text_embed_{i}'] = val\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example features\n",
        "sample_text = \"This is a sample sentence to demonstrate feature extraction.\"\n",
        "print(get_text_features(sample_text))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:03:58.178445Z",
          "iopub.execute_input": "2025-04-06T15:03:58.178771Z",
          "iopub.status.idle": "2025-04-06T15:03:58.609858Z",
          "shell.execute_reply.started": "2025-04-06T15:03:58.178732Z",
          "shell.execute_reply": "2025-04-06T15:03:58.609149Z"
        },
        "id": "rqMQQR47T4fR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_acoustic_features(audio_path):\n",
        "    # Load audio file\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    for i in range(13):\n",
        "        features[f'mfcc_{i}_mean'] = np.mean(mfcc[i])\n",
        "        features[f'mfcc_{i}_std'] = np.std(mfcc[i])\n",
        "\n",
        "    # Pitch features\n",
        "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
        "    pitches = pitches[pitches > 0]  # Remove zeros\n",
        "    if len(pitches) > 0:\n",
        "        features['pitch_mean'] = np.mean(pitches)\n",
        "        features['pitch_std'] = np.std(pitches)\n",
        "    else:\n",
        "        features['pitch_mean'] = 0\n",
        "        features['pitch_std'] = 0\n",
        "\n",
        "    # Energy features\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    features['energy_mean'] = np.mean(rms)\n",
        "    features['energy_std'] = np.std(rms)\n",
        "\n",
        "    # Speaking rate approximation\n",
        "    speech_duration = len(y) / sr\n",
        "    num_words = len(transcribe_audio(audio_path).split())\n",
        "    features['speaking_rate'] = num_words / speech_duration if speech_duration > 0 else 0\n",
        "\n",
        "    # Pause features\n",
        "    y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
        "    pause_ratio = (len(y) - len(y_trimmed)) / len(y)\n",
        "    features['pause_ratio'] = pause_ratio\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example features\n",
        "print(extract_acoustic_features(sample_audio))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:04:08.438487Z",
          "iopub.execute_input": "2025-04-06T15:04:08.438875Z",
          "iopub.status.idle": "2025-04-06T15:04:10.354617Z",
          "shell.execute_reply.started": "2025-04-06T15:04:08.438831Z",
          "shell.execute_reply": "2025-04-06T15:04:10.353703Z"
        },
        "id": "KmhJNCWnT4fR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files(df, audio_path, is_train=True):\n",
        "    features_list = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        audio_file = os.path.join(audio_path, row['filename'])\n",
        "\n",
        "        try:\n",
        "            # Step 1: Transcribe audio\n",
        "            transcription = transcribe_audio(audio_file)\n",
        "\n",
        "            # Step 2: Extract text features\n",
        "            text_features = get_text_features(transcription)\n",
        "\n",
        "            # Step 3: Extract acoustic features\n",
        "            acoustic_features = extract_acoustic_features(audio_file)\n",
        "\n",
        "            # Combine all features\n",
        "            combined_features = {**text_features, **acoustic_features}\n",
        "\n",
        "            # Add label if training data\n",
        "            if is_train:\n",
        "                combined_features['label'] = row['label']\n",
        "\n",
        "            combined_features['filename'] = row['filename']\n",
        "            features_list.append(combined_features)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {row['filename']}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(features_list)\n",
        "\n",
        "# Process training data\n",
        "print(\"Processing training data...\")\n",
        "train_features = process_files(train_df, train_audio_path)\n",
        "\n",
        "# Process test data\n",
        "print(\"\\nProcessing test data...\")\n",
        "test_features = process_files(test_df, test_audio_path, is_train=False)\n",
        "\n",
        "# Save features to avoid recomputing\n",
        "train_features.to_csv('train_features.csv', index=False)\n",
        "test_features.to_csv('test_features.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:04:10.355539Z",
          "iopub.execute_input": "2025-04-06T15:04:10.355803Z",
          "iopub.status.idle": "2025-04-06T15:55:32.600411Z",
          "shell.execute_reply.started": "2025-04-06T15:04:10.355778Z",
          "shell.execute_reply": "2025-04-06T15:55:32.599479Z"
        },
        "id": "2c1kpDsyT4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load features if not already in memory\n",
        "train_features = pd.read_csv('train_features.csv')\n",
        "test_features = pd.read_csv('test_features.csv')\n",
        "\n",
        "# Prepare data\n",
        "X = train_features.drop(['label', 'filename'], axis=1)\n",
        "y = train_features['label']\n",
        "\n",
        "# Split into train and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Train XGBoost model\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'eta': 0.01,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
        "dval = xgb.DMatrix(X_val_scaled, label=y_val)\n",
        "\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=50\n",
        ")\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_preds = model.predict(dval)\n",
        "pearson = pearsonr(y_val, val_preds)[0]\n",
        "print(f\"\\nValidation Pearson Correlation: {pearson:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "xgb.plot_importance(model, max_num_features=20, ax=ax)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sfvIDwxWT4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "X_test = test_features.drop(['filename'], axis=1)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "dtest = xgb.DMatrix(X_test_scaled)\n",
        "\n",
        "# Make predictions\n",
        "test_preds = model.predict(dtest)\n",
        "\n",
        "# Clip predictions to [1, 5] range\n",
        "# Round to 1 decimal place\n",
        "test_preds = np.round(np.clip(test_preds, 1, 5), 1)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_features['filename'],\n",
        "    'label': test_preds\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('xg_submission.csv', index=False)\n",
        "print(\"Submission file created!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:43:53.309894Z",
          "iopub.execute_input": "2025-04-06T16:43:53.310222Z",
          "iopub.status.idle": "2025-04-06T16:43:53.337567Z",
          "shell.execute_reply.started": "2025-04-06T16:43:53.310183Z",
          "shell.execute_reply": "2025-04-06T16:43:53.336761Z"
        },
        "id": "n38LHM6OT4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:43:56.558451Z",
          "iopub.execute_input": "2025-04-06T16:43:56.558737Z",
          "iopub.status.idle": "2025-04-06T16:43:56.567494Z",
          "shell.execute_reply.started": "2025-04-06T16:43:56.558714Z",
          "shell.execute_reply": "2025-04-06T16:43:56.566674Z"
        },
        "id": "JTCjJ_uQT4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define model\n",
        "def create_model(input_shape):\n",
        "    inputs = Input(shape=(input_shape,))\n",
        "\n",
        "    x = Dense(256, activation='relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    outputs = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and train model\n",
        "nn_model = create_model(X_train_scaled.shape[1])\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=20, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=10)\n",
        "]\n",
        "\n",
        "history = nn_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Evolution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.legend()\n",
        "plt.title('MAE Evolution')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "nn_val_preds = nn_model.predict(X_val_scaled).flatten()\n",
        "nn_pearson = pearsonr(y_val, nn_val_preds)[0]\n",
        "print(f\"\\nNeural Network Validation Pearson Correlation: {nn_pearson:.4f}\")\n",
        "\n",
        "# Make test predictions\n",
        "nn_test_preds = nn_model.predict(X_test_scaled).flatten()\n",
        "# Round to 1 decimal place\n",
        "nn_test_preds = np.round(np.clip(nn_test_preds, 1, 5), 1)\n",
        "\n",
        "# Create NN submission\n",
        "nn_submission = pd.DataFrame({\n",
        "    'filename': test_features['filename'],\n",
        "    'label': nn_test_preds\n",
        "})\n",
        "\n",
        "nn_submission.to_csv('nn_submission.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:43:59.519487Z",
          "iopub.execute_input": "2025-04-06T16:43:59.519782Z",
          "iopub.status.idle": "2025-04-06T16:44:08.651127Z",
          "shell.execute_reply.started": "2025-04-06T16:43:59.519759Z",
          "shell.execute_reply": "2025-04-06T16:44:08.650246Z"
        },
        "id": "w5OKb8P_T4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(\"/kaggle/working/xg_submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:43:42.000577Z",
          "iopub.execute_input": "2025-04-06T16:43:42.000868Z",
          "iopub.status.idle": "2025-04-06T16:43:42.004584Z",
          "shell.execute_reply.started": "2025-04-06T16:43:42.000844Z",
          "shell.execute_reply": "2025-04-06T16:43:42.003846Z"
        },
        "id": "By_oABKrT4fS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of regression model performance.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        Ground truth target values\n",
        "    y_pred : array-like\n",
        "        Predicted target values\n",
        "    model_name : str\n",
        "        Name of the model for reporting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing all calculated metrics\n",
        "    \"\"\"\n",
        "    # Round predictions to 2 decimal places for reporting\n",
        "    y_pred_rounded = np.round(y_pred, 2)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    pearson_corr, pearson_p = pearsonr(y_true, y_pred)\n",
        "    spearman_corr, spearman_p = spearmanr(y_true, y_pred)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 0.001))) * 100\n",
        "\n",
        "    # Within 0.5 and 1.0 accuracy\n",
        "    within_half = np.mean(np.abs(y_true - y_pred) < 0.5) * 100\n",
        "    within_one = np.mean(np.abs(y_true - y_pred) < 1.0) * 100\n",
        "\n",
        "    # Store metrics in dictionary\n",
        "    metrics = {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MAPE (%)': mape,\n",
        "        'R²': r2,\n",
        "        'Pearson Correlation': pearson_corr,\n",
        "        'Pearson p-value': pearson_p,\n",
        "        'Spearman Correlation': spearman_corr,\n",
        "        'Spearman p-value': spearman_p,\n",
        "        'Within 0.5 Accuracy (%)': within_half,\n",
        "        'Within 1.0 Accuracy (%)': within_one\n",
        "    }\n",
        "\n",
        "    # Print metrics in a formatted table\n",
        "    print(f\"\\n{model_name} Performance Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    for metric, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"{metric:25}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric:25}: {value}\")\n",
        "\n",
        "    # Create error visualization data\n",
        "    errors = y_true - y_pred\n",
        "    metrics['errors'] = errors\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Example usage for XGBoost model evaluation\n",
        "xgb_val_metrics = evaluate_model(y_val, val_preds, \"XGBoost (Validation)\")\n",
        "\n",
        "# Example usage for Neural Network model evaluation\n",
        "nn_val_metrics = evaluate_model(y_val, nn_val_preds, \"Neural Network (Validation)\")\n",
        "\n",
        "# Visualize error distributions\n",
        "def plot_error_analysis(metrics_dict1, metrics_dict2=None, model1_name=\"Model 1\", model2_name=\"Model 2\"):\n",
        "    \"\"\"\n",
        "    Plot error analysis visualizations for one or two models\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metrics_dict1 : dict\n",
        "        Dictionary with metrics from first model\n",
        "    metrics_dict2 : dict, optional\n",
        "        Dictionary with metrics from second model\n",
        "    model1_name : str\n",
        "        Name of first model\n",
        "    model2_name : str\n",
        "        Name of second model\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot error distributions\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(metrics_dict1['errors'], kde=True, color='blue', alpha=0.6, label=model1_name)\n",
        "    if metrics_dict2:\n",
        "        sns.histplot(metrics_dict2['errors'], kde=True, color='red', alpha=0.6, label=model2_name)\n",
        "    plt.axvline(x=0, color='black', linestyle='--')\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Error (True - Predicted)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot prediction vs actual scatter plot\n",
        "    plt.subplot(2, 2, 2)\n",
        "\n",
        "    # For first model\n",
        "    y_true1 = metrics_dict1['errors'] + y_val  # reconstructing y_true\n",
        "    y_pred1 = y_val - metrics_dict1['errors']  # reconstructing y_pred\n",
        "    plt.scatter(y_true1, y_pred1, alpha=0.5, color='blue', label=model1_name)\n",
        "\n",
        "    # For second model if provided\n",
        "    if metrics_dict2:\n",
        "        y_true2 = metrics_dict2['errors'] + y_val\n",
        "        y_pred2 = y_val - metrics_dict2['errors']\n",
        "        plt.scatter(y_true2, y_pred2, alpha=0.5, color='red', label=model2_name)\n",
        "\n",
        "    # Add perfect prediction line\n",
        "    min_val = min(y_true1)\n",
        "    max_val = max(y_true1)\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
        "    plt.title('Predictions vs Actual Values')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot residuals\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(y_val, metrics_dict1['errors'], alpha=0.5, color='blue', label=model1_name)\n",
        "    if metrics_dict2:\n",
        "        plt.scatter(y_val, metrics_dict2['errors'], alpha=0.5, color='red', label=model2_name)\n",
        "    plt.axhline(y=0, color='black', linestyle='--')\n",
        "    plt.title('Residual Plot')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot error boxplot by actual value range\n",
        "    plt.subplot(2, 2, 4)\n",
        "    bins = pd.cut(y_val, bins=5)\n",
        "\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    for bin_name, bin_group in pd.DataFrame({'bin': bins, 'error1': metrics_dict1['errors']}).groupby('bin'):\n",
        "        boxplot_data.append(bin_group['error1'].values)\n",
        "        labels.append(f\"{bin_name.left:.1f}-{bin_name.right:.1f}\")\n",
        "\n",
        "    plt.boxplot(boxplot_data, labels=labels)\n",
        "    plt.title('Error by Actual Value Range')\n",
        "    plt.xlabel('Actual Value Range')\n",
        "    plt.ylabel('Error')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize errors for both models\n",
        "plot_error_analysis(xgb_val_metrics, nn_val_metrics, \"XGBoost\", \"Neural Network\")\n",
        "\n",
        "# Model Comparison\n",
        "def compare_models(models_metrics, model_names):\n",
        "    \"\"\"\n",
        "    Create bar chart comparing key metrics across models\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models_metrics : list\n",
        "        List of metric dictionaries from different models\n",
        "    model_names : list\n",
        "        List of model names\n",
        "    \"\"\"\n",
        "    key_metrics = ['RMSE', 'MAE', 'R²', 'Pearson Correlation', 'Within 0.5 Accuracy (%)', 'Within 1.0 Accuracy (%)']\n",
        "\n",
        "    comparison_data = []\n",
        "    for i, metrics in enumerate(models_metrics):\n",
        "        for metric in key_metrics:\n",
        "            comparison_data.append({\n",
        "                'Model': model_names[i],\n",
        "                'Metric': metric,\n",
        "                'Value': metrics[metric]\n",
        "            })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, metric in enumerate(key_metrics):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
        "        sns.barplot(x='Model', y='Value', data=metric_data)\n",
        "        plt.title(metric)\n",
        "        plt.ylim(bottom=0 if metric != 'R²' and metric != 'Pearson Correlation' else None)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Compare models\n",
        "compare_models([xgb_val_metrics, nn_val_metrics], [\"XGBoost\", \"Neural Network\"])\n",
        "\n",
        "# Ensemble the models\n",
        "def create_ensemble(y_pred1, y_pred2, weights=[0.5, 0.5]):\n",
        "    \"\"\"\n",
        "    Create an ensemble of two models using weighted averaging\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_pred1 : array-like\n",
        "        Predictions from first model\n",
        "    y_pred2 : array-like\n",
        "        Predictions from second model\n",
        "    weights : list\n",
        "        Weights for averaging [weight1, weight2]\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    array-like\n",
        "        Ensemble predictions\n",
        "    \"\"\"\n",
        "    return weights[0] * y_pred1 + weights[1] * y_pred2\n",
        "\n",
        "# Try different ensemble weights to find the optimal combination\n",
        "def optimize_ensemble_weights(y_true, y_pred1, y_pred2, metric='pearson'):\n",
        "    \"\"\"\n",
        "    Find optimal weights for ensemble\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        Ground truth values\n",
        "    y_pred1 : array-like\n",
        "        Predictions from first model\n",
        "    y_pred2 : array-like\n",
        "        Predictions from second model\n",
        "    metric : str\n",
        "        Metric to optimize ('pearson', 'rmse', 'mae')\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (best_weight, best_score)\n",
        "    \"\"\"\n",
        "    weights = np.linspace(0, 1, 21)  # Try weights from 0 to 1 with 0.05 step\n",
        "    best_score = -np.inf if metric == 'pearson' else np.inf\n",
        "    best_weight = 0.5\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for w in weights:\n",
        "        ensemble_pred = create_ensemble(y_pred1, y_pred2, [w, 1-w])\n",
        "\n",
        "        if metric == 'pearson':\n",
        "            score = pearsonr(y_true, ensemble_pred)[0]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_weight = w\n",
        "        elif metric == 'rmse':\n",
        "            score = np.sqrt(mean_squared_error(y_true, ensemble_pred))\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_weight = w\n",
        "        elif metric == 'mae':\n",
        "            score = mean_absolute_error(y_true, ensemble_pred)\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_weight = w\n",
        "\n",
        "        scores.append((w, score))\n",
        "\n",
        "    # Plot weight optimization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    weights_list, scores_list = zip(*scores)\n",
        "    plt.plot(weights_list, scores_list, 'o-')\n",
        "    plt.axvline(x=best_weight, color='red', linestyle='--')\n",
        "    plt.xlabel('Weight for Model 1')\n",
        "    plt.ylabel(f'{metric.upper()} Score')\n",
        "    plt.title(f'Ensemble Weight Optimization (Best weight: {best_weight:.2f})')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return best_weight, best_score\n",
        "\n",
        "# Optimize ensemble weights\n",
        "best_weight, best_score = optimize_ensemble_weights(y_val, val_preds, nn_val_preds, 'pearson')\n",
        "print(f\"Best ensemble weight: {best_weight:.2f} for XGBoost, {1-best_weight:.2f} for Neural Network\")\n",
        "print(f\"Best Pearson correlation: {best_score:.4f}\")\n",
        "\n",
        "# Create ensemble predictions\n",
        "ensemble_val_preds = create_ensemble(val_preds, nn_val_preds, [best_weight, 1-best_weight])\n",
        "ensemble_val_metrics = evaluate_model(y_val, ensemble_val_preds, \"Ensemble (Validation)\")\n",
        "\n",
        "# Create final ensemble prediction for test set\n",
        "ensemble_test_preds = create_ensemble(test_preds, nn_test_preds, [best_weight, 1-best_weight])\n",
        "ensemble_test_preds = np.clip(ensemble_test_preds, 1, 5)\n",
        "\n",
        "# Create ensemble submission\n",
        "ensemble_submission = pd.DataFrame({\n",
        "    'filename': test_features['filename'],\n",
        "    'label': ensemble_test_preds.astype(int)  # optional: ensure integer type\n",
        "})\n",
        "\n",
        "ensemble_submission.to_csv('submission.csv', index=False)\n",
        "print(\"Ensemble submission file created!\")\n",
        "\n",
        "def plot_xgb_learning_curve(model, train_data, val_data):\n",
        "    \"\"\"\n",
        "    Plot learning curve for XGBoost model\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : xgb.Booster\n",
        "        Trained XGBoost model\n",
        "    train_data : xgb.DMatrix\n",
        "        Training data\n",
        "    val_data : xgb.DMatrix\n",
        "        Validation data\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # For a Booster object, get parameters directly from the model\n",
        "    params = model.get_params() if hasattr(model, 'get_params') else {}\n",
        "\n",
        "    # Get the best iteration or use default\n",
        "    best_iteration = model.best_iteration if hasattr(model, 'best_iteration') else 100\n",
        "\n",
        "    # Train model with evaluation at each iteration\n",
        "    xgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=best_iteration + 1,\n",
        "        evals=[(train_data, 'train'), (val_data, 'val')],\n",
        "        evals_result=results,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(results['train']['rmse'], label='Train RMSE')\n",
        "    plt.plot(results['val']['rmse'], label='Validation RMSE')\n",
        "    plt.axvline(x=best_iteration, color='red', linestyle='--',\n",
        "                label=f'Best iteration: {best_iteration}')\n",
        "    plt.xlabel('Number of Trees')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('XGBoost Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot XGBoost learning curve\n",
        "plot_xgb_learning_curve(model, dtrain, dval)\n",
        "\n",
        "# Feature importance analysis with more detail\n",
        "def analyze_feature_importance(model, feature_names):\n",
        "    \"\"\"\n",
        "    Detailed analysis of feature importance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : xgb.Booster\n",
        "        Trained XGBoost model\n",
        "    feature_names : list\n",
        "        Names of features\n",
        "    \"\"\"\n",
        "    # Get importance scores\n",
        "    importance_dict = model.get_score(importance_type='gain')\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': list(importance_dict.keys()),\n",
        "        'Importance': list(importance_dict.values())\n",
        "    })\n",
        "\n",
        "    # Sort by importance\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Calculate cumulative importance\n",
        "    importance_df['Normalized'] = importance_df['Importance'] / importance_df['Importance'].sum()\n",
        "    importance_df['Cumulative'] = importance_df['Normalized'].cumsum()\n",
        "\n",
        "    # Group features by type\n",
        "    feature_types = {\n",
        "        'text_embed': 'Text Embedding',\n",
        "        'mfcc': 'MFCC Audio',\n",
        "        'pitch': 'Pitch',\n",
        "        'energy': 'Energy',\n",
        "        'speaking_rate': 'Speaking Rate',\n",
        "        'pause': 'Pauses',\n",
        "        'num_words': 'Text Stats',\n",
        "        'num_chars': 'Text Stats',\n",
        "        'avg_word': 'Text Stats',\n",
        "        'unique_word': 'Text Stats'\n",
        "    }\n",
        "\n",
        "    # Assign type to each feature\n",
        "    def get_feature_type(feature_name):\n",
        "        for key, feature_type in feature_types.items():\n",
        "            if key in feature_name.lower():\n",
        "                return feature_type\n",
        "        return 'Other'\n",
        "\n",
        "    importance_df['Type'] = importance_df['Feature'].apply(get_feature_type)\n",
        "\n",
        "    # Plot importance by feature\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
        "    plt.title('Top 20 Features by Importance')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plot importance by feature type\n",
        "    type_importance = importance_df.groupby('Type')['Importance'].sum().reset_index()\n",
        "    type_importance = type_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.barplot(x='Importance', y='Type', data=type_importance)\n",
        "    plt.title('Feature Importance by Type')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print top features\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    for i, row in importance_df.head(10).iterrows():\n",
        "        print(f\"{i+1}. {row['Feature']}: {row['Normalized']:.4f} ({row['Normalized']*100:.2f}%)\")\n",
        "\n",
        "    # Print importance by type\n",
        "    print(\"\\nFeature Importance by Type:\")\n",
        "    for i, row in type_importance.iterrows():\n",
        "        total_importance = row['Importance'] / importance_df['Importance'].sum()\n",
        "        print(f\"{row['Type']}: {total_importance:.4f} ({total_importance*100:.2f}%)\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Analyze feature importance\n",
        "feature_importance = analyze_feature_importance(model, X.columns)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:47:05.75468Z",
          "iopub.execute_input": "2025-04-06T16:47:05.755Z",
          "iopub.status.idle": "2025-04-06T16:47:15.859982Z",
          "shell.execute_reply.started": "2025-04-06T16:47:05.754974Z",
          "shell.execute_reply": "2025-04-06T16:47:15.859122Z"
        },
        "id": "1Hr2381KT4fT"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}